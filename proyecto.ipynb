{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://7612750.fs1.hubspotusercontent-na1.net/hubfs/7612750/logo%20Senpai_2022%20(1)-1.png)\n",
    "# Proyecto final - Bootcamp Data Science\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas utilizadas\n",
    "\n",
    "*   pandas\n",
    "*   NumPy\n",
    "*   matplotlib\n",
    "*   plotly\n",
    "*   h5py\n",
    "*   pickle\n",
    "*   XGBoost\n",
    "*   prophet\n",
    "*   TensorFlow\n",
    "*   scikit-learn\n",
    "*   Keras\n",
    "*   Neptune\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preámbulo\n",
    "\n",
    "Esta notebook tiene el propósito de seleccionar el modelo de regresión con series temporales más adecuado, que realice un pronóstico de la ganancia de la empresa C.O.M.E.S.A en el mes de agosto de 2023.\n",
    "\n",
    "## Datos\n",
    "\n",
    "Los datos a usar son los archivos csv obtenidos de la ejecución de la notebook \"datos_a_usar.ipynb\".\n",
    "\n",
    " Dichos archivos son llamados de la forma ***viajes_stm_0x2023_comesa.csv*** (donde x se cambia por el mes en cuestión)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones y librerías\n",
    "\n",
    "En el siguiente apartado se definen las funciones y se realizan las importaciones de las librerías aplicadas al desarrollo del proyecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import h5py\n",
    "import pickle\n",
    "import neptune\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "from neptune.exceptions import NeptuneModelKeyAlreadyExistsError\n",
    "from prophet import Prophet\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def borrar_nulos(df):\n",
    "  \"\"\"Borra las filas con valores faltantes de un dataframe.\n",
    "\n",
    "  Args:\n",
    "        param1: df (pandas.core.frame.DataFrame)\n",
    "\n",
    "  Returns:\n",
    "        output1: df (pandas.core.frame.DataFrame)\n",
    "  \"\"\"\n",
    "  df = df.dropna()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacion_no_nulos(lista_dfs):\n",
    "    \"\"\"Valida que ningún dataframe de una lista de dataframes quede con valores nulos.\n",
    "        Devuelve True cuando se cumple la condición.\n",
    "\n",
    "    Args:\n",
    "        param1: lista_dfs (list)\n",
    "\n",
    "    Returns:\n",
    "        output1: bool\n",
    "    \"\"\"\n",
    "    hay_nulos = False\n",
    "    for df in lista_dfs:\n",
    "        hay_nulos = df.isna().any().any()\n",
    "    return hay_nulos == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Las tarifas se obtuvieron de:\n",
    "https://montevideo.gub.uy/areas-tematicas/sistema-de-transporte-metropolitano/tarifas-del-transporte-colectivo-urbano\n",
    "\n",
    "La forma de aplicar esta función se obtuvo de:\n",
    "https://medium.com/@michalwesleymnach/the-complete-guide-to-create-columns-based-on-multiple-conditions-in-pandas-dataframes-eedf2c0392a6\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def agregarImporte(con_tarjeta, tipo_viaje, cantidad_pasajeros):\n",
    "    \"\"\"Devuelve el importe total de un viaje teniendo en cuenta si el viaje es\n",
    "        con tarjeta, qué tipo de viaje es y la cantidad de pasajeros.\n",
    "\n",
    "    Args:\n",
    "        param1: con_tarjeta (int64)\n",
    "        param2: tipo_viaje (float64)\n",
    "        param3: cantidad_pasajeros (int64)\n",
    "\n",
    "    Returns:\n",
    "        output1: float64\n",
    "    \"\"\"\n",
    "    #tipo_viaje 12 y 14 no existen\n",
    "\n",
    "    importe = 0\n",
    "\n",
    "    if tipo_viaje == 1:\n",
    "      #Viaje común\n",
    "      importe = 52\n",
    "    elif tipo_viaje == 2:\n",
    "      #Viaje zona\n",
    "      if con_tarjeta == 1:\n",
    "        importe = 20\n",
    "      else:\n",
    "        importe = 27\n",
    "    elif tipo_viaje == 3:\n",
    "      #Viaje céntrico\n",
    "      if con_tarjeta == 1:\n",
    "        importe = 29\n",
    "      else:\n",
    "        importe = 39\n",
    "    elif tipo_viaje == 4:\n",
    "      #Viaje diferencial\n",
    "      if con_tarjeta == 1:\n",
    "        importe = 62\n",
    "      else:\n",
    "        importe = 78\n",
    "    elif tipo_viaje == 5:\n",
    "      #Viaje una hora\n",
    "      if con_tarjeta == 1:\n",
    "        importe = 42\n",
    "      else:\n",
    "        importe = 52\n",
    "    elif tipo_viaje == 6:\n",
    "      #Viaje dos horas\n",
    "      if con_tarjeta == 1:\n",
    "        importe = 62\n",
    "      else:\n",
    "        importe = 78\n",
    "    elif tipo_viaje == 7:\n",
    "      #Viaje estudiante categoría A\n",
    "        importe = 23\n",
    "    elif tipo_viaje == 8:\n",
    "      #Viaje estudiante categoría B\n",
    "        importe = 32.2\n",
    "    elif tipo_viaje == 9:\n",
    "      #Viaje estudiante gratuito\n",
    "        importe = 0\n",
    "    elif tipo_viaje == 10:\n",
    "      #Viaje jubilado o pensionista categoría A\n",
    "      if con_tarjeta == 1:\n",
    "        importe = 11\n",
    "      else:\n",
    "        importe = 14\n",
    "    elif tipo_viaje == 11:\n",
    "      #Viaje jubilado o pensionista categoría B\n",
    "      if con_tarjeta == 1:\n",
    "        importe = 19\n",
    "      else:\n",
    "        importe = 22\n",
    "    elif tipo_viaje == 13:\n",
    "      #Viaje combinación metropolitana\n",
    "      importe = 66\n",
    "    elif tipo_viaje == 15:\n",
    "      #Viaje gratuito\n",
    "      importe = 0\n",
    "    elif tipo_viaje == 16:\n",
    "      #Boleto ABC\n",
    "      importe = 33\n",
    "    elif tipo_viaje == 17:\n",
    "      #Viaje prepago nominado\n",
    "      importe = 38.7\n",
    "    elif tipo_viaje == 18:\n",
    "      #Viaje para usuario/a frecuente\n",
    "      importe = 39\n",
    "    elif tipo_viaje == 19:\n",
    "      #Trab. Med. y larga dist.\n",
    "      importe = 0\n",
    "\n",
    "    return importe*cantidad_pasajeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acondicionarDf(df):\n",
    "  \"\"\"Realiza ajustes para acondicionar el dataset.\n",
    "\n",
    "  Args:\n",
    "      param1: df (pandas.core.frame.DataFrame)\n",
    "\n",
    "  Returns:\n",
    "      output1: dfaux (pandas.core.frame.DataFrame)\n",
    "  \"\"\"\n",
    "\n",
    "  dfaux = df.copy()\n",
    "\n",
    "  # La columna dsc_linea venía con tipos de datos mezclados, se convierte a str\n",
    "  dfaux['dsc_linea'] = dfaux['dsc_linea'].astype(str)\n",
    "\n",
    "  # Crea la columna importe y le asigna los valores correspondientes\n",
    "  func = np.vectorize(agregarImporte)\n",
    "  dfaux[\"importe\"] = func(dfaux[\"con_tarjeta\"], dfaux[\"tipo_viaje\"], dfaux[\"cantidad_pasajeros\"])\n",
    "\n",
    "  # Convierte fecha_evento a datetime\n",
    "  dfaux['fecha_evento'] = pd.to_datetime(dfaux['fecha_evento'])\n",
    "\n",
    "  # Le quita los minutos y los segundos a la hora\n",
    "  dfaux['fecha_evento'] = pd.to_datetime(dfaux['fecha_evento'].dt.strftime('%Y-%m-%d %H:00:00'))\n",
    "\n",
    "  # Agrupa por fecha y sumariza los importes\n",
    "  dfaux = dfaux.groupby(dfaux.fecha_evento)[['importe']].sum()\n",
    "\n",
    "  # El group by genera multiindices, usando reset_index pasan a ser columnas\n",
    "  dfaux = dfaux.reset_index()\n",
    "\n",
    "  return dfaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lista_dfs_preparados(lista_dfs):\n",
    "  \"\"\"Recibe una lista de dataframes y\n",
    "      devuelve una lista contienendo los dataframes preparados.\n",
    "\n",
    "  Args:\n",
    "      param1: lista_dfs (list)\n",
    "\n",
    "  Returns:\n",
    "      output1: lista_dfs_preparados (list)\n",
    "  \"\"\"\n",
    "\n",
    "  lista_dfs_preparados = []\n",
    "  for df in lista_dfs:\n",
    "    df_i = acondicionarDf(df)\n",
    "    lista_dfs_preparados.append(df_i)\n",
    "\n",
    "  return lista_dfs_preparados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta función fue tomada de:\n",
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "\t\"\"\"Divide una secuencia flatten en muchas muestras cada una de las cuales\n",
    "\t\t\ttiene un número especificado (n_steps) de datestamps.\n",
    "\t\t\tDevuelve el array 2D obtenido como se especifica y un array 1D\n",
    "\t\t\tque tiene un solo datestamp.\n",
    "\n",
    "  Args:\n",
    "      param1: sequence (pandas.core.series.Series)\n",
    "\t\t\tparam2: n_steps (int)\n",
    "\n",
    "  Returns:\n",
    "      output1: numpy.ndarray\n",
    "\t\t\toutput2: numpy.ndarray\n",
    "  \"\"\"\n",
    "\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafica_residuos_neptune(name_model,y_true, y_pred):\n",
    "  \"\"\"Grafica los residuos en función del tiempo.\n",
    "\n",
    "  Args:\n",
    "      param1: name_model (str)\n",
    "\t\t\tparam2: y_true (numpy.ndarray)\n",
    "      param3: y_pred (numpy.ndarray)\n",
    "\n",
    "  \"\"\"\n",
    "  mse = mean_squared_error(y_true, y_pred)\n",
    "  mae = mean_absolute_error(y_true, y_pred)\n",
    "  r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "  run[\"evaluation/{}/mse\".format(name_model)] = mse\n",
    "  run[\"evaluation/{}/mae\".format(name_model)] = mae\n",
    "  run[\"evaluation/{}/r2\".format(name_model)] = r2\n",
    "\n",
    "  residuals = y_true - y_pred\n",
    "  matriz_residuals = np.vstack((np.arange(len(residuals)), residuals))\n",
    "  matriz_residuals = np.transpose(matriz_residuals)\n",
    "  residuals_df = pd.DataFrame(matriz_residuals, columns=['Tiempo','Residuos'])\n",
    "\n",
    "  fig = px.line(residuals_df, x=\"Tiempo\", y=\"Residuos\", title='Modelo: {}'.format(name_model) + '      '\n",
    "  \"MSE: %.3f MAE: %.3f R2: %.3f\" % (mse,mae,r2))\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacion_cruzada(model,X,y,tscv):\n",
    "  \"\"\"Realiza validación cruzada en una serie temporal para el modelo (model)\n",
    "      con el conjunto de datos de train (X) y los targets (y)\n",
    "\n",
    "  Args:\n",
    "      param1: model\n",
    "\t\t\tparam2: X (numpy.ndarray)\n",
    "      param3: y (numpy.ndarray)\n",
    "      param4: tscv (sklearn.model_selection._split.TimeSeriesSplit)\n",
    "\n",
    "  Returns:\n",
    "      output1: numpy.ndarray\n",
    "\t\t\toutput2: numpy.ndarray\n",
    "  \"\"\"\n",
    "\n",
    "  y_pred_val = np.array([])\n",
    "  y_val = np.array([])\n",
    "\n",
    "  # Realizar la validación cruzada manualmente y obtener predicciones\n",
    "  for train_index, val_index in tscv.split(X):\n",
    "\n",
    "    X_training, X_validation = X[train_index], X[val_index]\n",
    "    y_training, y_validation = y[train_index], y[val_index]\n",
    "\n",
    "    y_val = np.concatenate([y_val,y_validation])\n",
    "\n",
    "    model.fit(X_training, y_training)\n",
    "\n",
    "    y_pred_fold = model.predict(X_validation)\n",
    "    y_pred_val = np.concatenate([y_pred_val, y_pred_fold])\n",
    "\n",
    "  return y_val,y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "La función fue extraída de la documentación de TensorFlow tf.data.Dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data(data, lookback, batch_size):\n",
    "  \"\"\"Convierte los datos a un tf.data.Dataset y los prepara para la LSTM.\n",
    "\n",
    "  Args:\n",
    "      param1: data (pandas.core.series.Series)\n",
    "\t\t\tparam2: lookback (int)\n",
    "      param3: batch_size (int)\n",
    "\n",
    "  Returns:\n",
    "      output1: dataset (tf.data.Dataset)\n",
    "  \"\"\"\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(data.values)\n",
    "  dataset = dataset.window(lookback + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(lookback + 1))\n",
    "  dataset = dataset.map(lambda window: (window[:-1][..., np.newaxis], window[-1]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completar con la ruta a los csv obtenidos de la notebook \"datos_a_usar.ipynb\"\n",
    "# si no se hace ninguna modificación luego de ejecutar la notebook \"datos_a_usar.ipynb\", el base_path debería ser '' \n",
    "\n",
    "base_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(base_path + '/data/viajes_stm_022023_comesa.csv')\n",
    "df_3 = pd.read_csv(base_path + '/data/viajes_stm_032023_comesa.csv')\n",
    "df_4 = pd.read_csv(base_path + '/data/viajes_stm_042023_comesa.csv')\n",
    "df_5 = pd.read_csv(base_path + '/data/viajes_stm_052023_comesa.csv')\n",
    "df_6 = pd.read_csv(base_path + '/data/viajes_stm_062023_comesa.csv')\n",
    "df_7 = pd.read_csv(base_path + '/data/viajes_stm_072023_comesa.csv')\n",
    "df_8 = pd.read_csv(base_path + '/data/viajes_stm_082023_comesa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Febrero:', '\\n', df_2.isna().sum())\n",
    "print('\\n')\n",
    "print('Marzo:', '\\n', df_3.isna().sum())\n",
    "print('\\n')\n",
    "print('Abril:', '\\n', df_4.isna().sum())\n",
    "print('\\n')\n",
    "print('Mayo:', '\\n', df_5.isna().sum())\n",
    "print('\\n')\n",
    "print('Junio:', '\\n', df_6.isna().sum())\n",
    "print('\\n')\n",
    "print('Julio:', '\\n', df_7.isna().sum())\n",
    "print('\\n')\n",
    "print('Agosto:', '\\n',  df_8.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nota: Se decidió eliminar los nulos en todos los dataframes porque como mucho habían 10 en un dataframe de más de 2 millones de filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = borrar_nulos(df_3)\n",
    "df_6 = borrar_nulos(df_6)\n",
    "df_7 = borrar_nulos(df_7)\n",
    "df_8 = borrar_nulos(df_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dfs = [df_2,df_3,df_4,df_5,df_6,df_7,df_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si devuelve True estamos en la condición de que no hay ningún valor faltante en ningún dataframe\n",
    "validacion_no_nulos(lista_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de datasets a partir de lista de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dfs_preparados = lista_dfs_preparados(lista_dfs[:-1])\n",
    "df_train = pd.concat(lista_dfs_preparados, ignore_index=True)\n",
    "\n",
    "df_train = df_train[['fecha_evento', 'importe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = acondicionarDf(df_8)\n",
    "\n",
    "df_test = df_test[['fecha_evento', 'importe']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de las matrices de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = split_sequence(df_train['importe'], 24)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = split_sequence(df_test['importe'], 24)\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_train, x=\"fecha_evento\", y=\"importe\", title='Ganancia en los meses desde febrero hasta julio del 2023')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observaciones:** Se puede observar en el gráfico que la serie temporal de estudio tiene un comportamiento bastante periódico en los meses desde febrero 2023 hasta julio 2023. Tiene componentes estacionales, feriados o vacaciones, que se pueden ver pero sin contar esos casos se logra apreciar la periodicidad de la serie temporal.\n",
    "Esto indica que los modelos de forecasting podrían funcionar bien debido a que pueden capturar y aprender ese patrón periódico para hacer predicciones futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos predictivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar clase de TimeSeriesSplit para hacer validación cruzada\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización / Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "y_train_norm = normalizer.fit_transform(y_train.reshape(-1, 1))\n",
    "y_train_norm = y_train_norm.reshape((len(y_train_norm)))\n",
    "\n",
    "y_test_norm = normalizer.transform(y_test.reshape(-1, 1))\n",
    "y_test_norm = y_test_norm.reshape((len(y_test_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión con Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\",\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\": 1000\n",
    "}\n",
    "\n",
    "run[\"model_RF/parameters\"] = parameters\n",
    "\n",
    "try:\n",
    "    model_RF_Neptune = neptune.init_model(\n",
    "    name=\"Random Forest\",\n",
    "    key=\"MOD\",\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\", # your credentials\n",
    "    )\n",
    "\n",
    "except NeptuneModelKeyAlreadyExistsError:\n",
    "    # If it already exists, we don't have to do anything.\n",
    "    pass\n",
    "\n",
    "model_RF_Neptune = neptune.init_model_version(\n",
    "      model=\"PROY-MOD\",\n",
    "      project=\"Senpai/ProyectoFinal\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYzgzMDUzYS01NTQ1LTQ2M2EtODMyMi0xOTJkY2ZmZGZjZGEifQ==\", # your credentials\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestRegressor(n_estimators = parameters[\"n_estimators\"], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val, y_pred_val = validacion_cruzada(model_RF,X_train_norm,y_train_norm,tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafica_residuos_neptune('Random Forest', y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observaciones: En el gráfico de validación de Random Forest (validación cruzada) se aprecia que el modelo mejora sus predicciones a medida que pasa el tiempo. Esta tendencia se observa en que los residuos disminuyen a medida que avanza la línea temporal. Debido a que el modelo ha sido entrenado cada vez con más datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo y subirlo a Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/models/best_model_RF.pkl', 'wb') as f:\n",
    "    pickle.dump(model_RF, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF_Neptune[\"model_RF\"].upload(base_path + \"/models/best_model_RF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG-Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización / Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "y_train_norm = normalizer.fit_transform(y_train.reshape(-1, 1))\n",
    "y_train_norm = y_train_norm.reshape((len(y_train_norm)))\n",
    "\n",
    "y_test_norm = normalizer.transform(y_test.reshape(-1, 1))\n",
    "y_test_norm = y_test_norm.reshape((len(y_test_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión con Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\",\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\": 1000\n",
    "}\n",
    "\n",
    "run[\"model_XGB/parameters\"] = parameters\n",
    "\n",
    "try:\n",
    "    model_XGB_Neptune = neptune.init_model(\n",
    "    name=\"XGBoost\",\n",
    "    key=\"MOD\",\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\", # your credentials\n",
    "    )\n",
    "\n",
    "except NeptuneModelKeyAlreadyExistsError:\n",
    "    # If it already exists, we don't have to do anything.\n",
    "    pass\n",
    "\n",
    "model_XGB_Neptune = neptune.init_model_version(\n",
    "      model=\"PROY-MOD\",\n",
    "      project=\"Senpai/ProyectoFinal\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYzgzMDUzYS01NTQ1LTQ2M2EtODMyMi0xOTJkY2ZmZGZjZGEifQ==\", # your credentials\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB = xgb.XGBRegressor(n_estimators=parameters['n_estimators'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val, y_pred_val = validacion_cruzada(model_XGB,X_train_norm,y_train_norm,tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape, y_pred_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafica_residuos_neptune('XGBoost', y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observaciones: Así como en Random Forest, en el gráfico de validación de XGBoost (validación cruzada) se aprecia que el modelo mejora sus predicciones a medida que pasa el tiempo. Esta tendencia se observa en que los residuos disminuyen a medida que avanza la línea temporal. Debido a que el modelo ha sido entrenado cada vez con más datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo y subirlo a Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/models/best_model_XGB.pkl', 'wb') as f:\n",
    "    pickle.dump(model_XGB, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB_Neptune[\"model_XGB\"].upload(base_path + \"/models/best_model_XGB.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización / Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_train = df_train.copy()\n",
    "df_lstm_test = df_test.copy()\n",
    "\n",
    "df_training = df_lstm_train.iloc[:3000]\n",
    "df_val = df_lstm_train.iloc[3000:]\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "df_training['importe'] = scaler.fit_transform(df_training[['importe']])\n",
    "df_val['importe'] = scaler.transform(df_val[['importe']])\n",
    "df_lstm_test['importe'] = scaler.transform(df_lstm_test[['importe']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos de entrenamiento, validación y testeo\n",
    "lookback = 2  # Define valor de lookback (ventana hacia atrás para predicciones)\n",
    "batch_size = 12  # Define valor de batch_size\n",
    "horizon = 1 # Cantidad de pasos a predecir\n",
    "\n",
    "train_data = prepare_data(df_training['importe'], lookback, batch_size)\n",
    "val_data = prepare_data(df_val['importe'], lookback, batch_size)\n",
    "test_data = prepare_data(df_lstm_test['importe'], lookback, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión con Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\",\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"dense_units\": 4,\n",
    "    \"batch_size\": 12,\n",
    "    \"n_epochs\": 40,\n",
    "}\n",
    "\n",
    "run[\"model_LSTM/parameters\"] = parameters\n",
    "\n",
    "try:\n",
    "    model_LSTM_Neptune = neptune.init_model(\n",
    "    name=\"LSTM\",\n",
    "    key=\"MOD\",\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\", # your credentials\n",
    "    )\n",
    "\n",
    "except NeptuneModelKeyAlreadyExistsError:\n",
    "    # If it already exists, we don't have to do anything.\n",
    "    pass\n",
    "\n",
    "model_LSTM_Neptune = neptune.init_model_version(\n",
    "      model=\"PROY-MOD\",\n",
    "      project=\"Senpai/ProyectoFinal\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYzgzMDUzYS01NTQ1LTQ2M2EtODMyMi0xOTJkY2ZmZGZjZGEifQ==\", # your credentials\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(lookback, 1), return_sequences = True))\n",
    "model.add(LSTM(64, return_sequences = True))\n",
    "model.add(LSTM(32,))\n",
    "model.add(Dense(horizon))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=10,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='best_model_LSTM.h5',\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "neptune_callback = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "\n",
    "callbacks = [early_stopping,checkpoint,neptune_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data,\n",
    "                    epochs=parameters['n_epochs'],\n",
    "                    validation_data=val_data,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observaciones: De la gráfica de entrenamiento del modelo se visualiza que los valores de pérdida descienden rápidamente luego de pocas épocas. El modelo capturó las tendencias y los patrones de los datos sin tender al sobreajuste. Lo que indica un ajuste adecuado de los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.array(df_val[['importe']][2:])\n",
    "y_val = y_val.reshape(len(y_val))\n",
    "\n",
    "y_pred_val = model.predict(val_data)\n",
    "y_pred_val = y_pred_val.reshape(len(y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafica_residuos_neptune('LSTM', y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observaciones: En el gráfico de validación de LSTM se ve que el modelo mantiene valores bajos residuales, sin embargo no se nota una disminución de los residuos a lo largo del entrenamiento, como sí se veía en los modelos de Random Forest y XGBoost. Una posible  explicación de este hecho es que no se hizo validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo y subirlo a Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(base_path + '/models/best_model_LSTM.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM_Neptune[\"model_LSTM\"].upload(base_path + \"/models/best_model_LSTM.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado logarítmico\n",
    "df_prophet_train = df_train.copy()\n",
    "df_prophet_test = df_test.copy()\n",
    "\n",
    "df_prophet_train['importe'] = np.log1p(df_prophet_train['importe'])\n",
    "df_prophet_test['importe'] = np.log1p(df_prophet_test['importe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_prophet_train.iloc[:3000]\n",
    "val = df_prophet_train.iloc[3000:]\n",
    "\n",
    "df_prophet_train = train\n",
    "df_prophet_train.columns = ['ds', 'y']\n",
    "\n",
    "df_prophet_val = val\n",
    "df_prophet_val.columns = ['ds', 'y']\n",
    "\n",
    "df_prophet_test.columns = ['ds', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión con Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\",\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"periods\": 1317,\n",
    "    \"freq\": \"H\"\n",
    "}\n",
    "\n",
    "run[\"model_Prophet/parameters\"] = parameters\n",
    "\n",
    "try:\n",
    "    model_Prophet_Neptune = neptune.init_model(\n",
    "    name=\"Prophet\",\n",
    "    key=\"MOD\",\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\", # your credentials\n",
    "    )\n",
    "\n",
    "except NeptuneModelKeyAlreadyExistsError:\n",
    "    # If it already exists, we don't have to do anything.\n",
    "    pass\n",
    "\n",
    "model_Prophet_Neptune = neptune.init_model_version(\n",
    "      model=\"PROY-MOD\",\n",
    "      project=\"Senpai/ProyectoFinal\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYzgzMDUzYS01NTQ1LTQ2M2EtODMyMi0xOTJkY2ZmZGZjZGEifQ==\", # your credentials\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet()\n",
    "\n",
    "m.fit(df_prophet_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_val = m.make_future_dataframe(periods=parameters[\"periods\"], freq=parameters[\"freq\"])\n",
    "future_val = future_val.tail(len(df_prophet_val))\n",
    "future_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_val = m.predict(future_val)\n",
    "forecast_val[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = df_prophet_val['y'].values\n",
    "y_pred_val = forecast_val['yhat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafica_residuos_neptune('Prophet', y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observaciones: En el gráfico de validación de Prophet también se ve que el modelo mantiene valores bajos residuales y tampoco se nota una disminución de los residuos a lo largo del entrenamiento, como sí se veía en los modelos de Random Forest y XGBoost. Una posible  explicación de este hecho es que no se hizo validación cruzada. Además se puede apreciar como Prophet aprende los patrones en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plotly(m, forecast_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observaciones: El gráfico muestra en negro los valores de entrenamiento y en azul las predicciones. Nuevamente se observa que Prophet aprende los patrones de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components_plotly(m, forecast_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observaciones: El primer gráfico muestra las tendencias a largo plazo ignorando la estacionalidad. Los otros dos nos permiten visualizar patrones semanales y diarios. Estas gráficas son útiles para entender cómo el modelo está interpretando los datos y qué factores están influyendo en los predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(forecast_val['ds'], y_val, label='Valores Reales', marker='o', color='black')\n",
    "plt.plot(forecast_val['ds'], y_pred_val, label='Predicciones', color='blue')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Comparación entre Predicciones y Valores Reales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observaciones: En la gráfica se superponen los valores reales con las predicciones, lo que nos permite ver que tan cercanos están entre sí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo y subirlo a Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/models/best_model_Prophet.pkl', 'wb') as f:\n",
    "    pickle.dump(m, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Prophet_Neptune[\"model_Prophet\"].upload(base_path + '/models/best_model_Prophet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del motivo de la elección del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegir el modelo más adecuado es un paso crucial para obtener buenos resultados. Según el análisis, de los cuatro modelos evaluados, Prophet sería la mejor elección, ya que está diseñado para trabajar con series temporales que tienen efectos estacionales fuertes y tendencias que son difíciles de modelar con otros métodos.\n",
    "\n",
    "Razones por las cuales se elige Prophet:\n",
    "\n",
    "* **Facilidad de uso**: Prophet es fácil de usar y requiere menos ajustes de hiperparámetros en comparación con otros modelos.\n",
    "\n",
    "* **Manejo de estacionalidades**: Prophet maneja automáticamente múltiples estacionalidades (diaria, semanal, anual) y los efectos de días festivos.\n",
    "\n",
    "* **Tendencias flexibles**: Prophet permite modelar cambios en las tendencias a lo largo del tiempo, lo cual fue útil dado que los datos utilizados tenían muchos puntos de cambio y una tendencia no lineal.\n",
    "\n",
    "* **Predicciones y intervalos de confianza**: Prophet genera predicciones futuras con intervalos de confianza, lo que lleva a una mayor confiabilidad en los resultados obtenidos.\n",
    "\n",
    "* **Eficiencia**: Prophet tiene el mejor balance entre tiempo de ejecución y resultados obtenidos. Además requiere poca capacidad de cómputo.\n",
    "\n",
    "* **Binario liviano**: Comparado con los otros modelos, el tamaño del archivo generado es mucho menor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance modelo elegido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df_train.copy()\n",
    "data_train['importe'] = np.log1p(data_train['importe'])\n",
    "data_train.columns = ['ds', 'y']\n",
    "\n",
    "data_test = df_test.copy()\n",
    "data_test['importe'] = np.log1p(data_test['importe'])\n",
    "data_test.columns = ['ds', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_elegido = Prophet()\n",
    "\n",
    "modelo_elegido.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión con Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\",\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"periods\": 744,\n",
    "    \"freq\": \"H\"\n",
    "}\n",
    "\n",
    "run[\"model_prod/parameters\"] = parameters\n",
    "\n",
    "try:\n",
    "    model_prod_Neptune = neptune.init_model(\n",
    "    name=\"Model Prod\",\n",
    "    key=\"MOD\",\n",
    "    project=\"Senpai/ProyectoFinal\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MjJmNGI3Zi1hOTIxLTQyYmMtYjg2OC0wODUyNTVjOTRlMmUifQ==\", # your credentials\n",
    "    )\n",
    "\n",
    "except NeptuneModelKeyAlreadyExistsError:\n",
    "    # If it already exists, we don't have to do anything.\n",
    "    pass\n",
    "\n",
    "model_prod_Neptune = neptune.init_model_version(\n",
    "      model=\"PROY-MOD\",\n",
    "      project=\"Senpai/ProyectoFinal\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYzgzMDUzYS01NTQ1LTQ2M2EtODMyMi0xOTJkY2ZmZGZjZGEifQ==\", # your credentials\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = modelo_elegido.make_future_dataframe(periods=parameters[\"periods\"], freq=parameters[\"freq\"])\n",
    "future = future.tail(len(data_test))\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = modelo_elegido.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = data_test['y'].values\n",
    "y_pred = forecast['yhat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafica_residuos_neptune('Prophet', y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plotly(modelo_elegido, forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components_plotly(modelo_elegido, forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(forecast['ds'], y_test, label='Valores Reales', marker='o', color='black')\n",
    "plt.plot(forecast['ds'], y_pred, label='Predicciones', color='blue')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Comparación entre Predicciones y Valores Reales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar modelo y subirlo a Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/models/best_model_prod.pkl', 'wb') as f:\n",
    "    pickle.dump(modelo_elegido, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prod_Neptune[\"model_prod\"].upload(base_path + '/models/best_model_prod.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En síntesis, la aplicación de modelos de series temporales puede proporcionar herramientas valiosas para el análisis de aspectos del transporte público. A través de un proceso riguroso, se eligió Prophet como el más adecuado, dada su capacidad de reconocer patrones, manejo de estacionalidad y otras bondades antes mencionadas. En comparación con los otros modelos evaluados, Prophet demostró ser superior en términos de equilibrio entre precisión, complejidad y tiempo de ejecución.\n",
    "\n",
    "Con este proyecto se pudo reconocer la importancia de la comprensión del contexto del problema, el preprocesamiento de los datos y la selección de los modelos para resolver la problemática.\n",
    "\n",
    "Cabe destacar que este proyecto tiene aplicación en casos reales, usando el modelo elegido se puede predecir la cantidad de meses hacia el futuro que se desee. Además, esto mismo se podría implementar para otras empresas que tengan datos similares lo que brinda gran flexibilidad a la hora de aplicar este modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
